{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srijita264/HAR-using-image-classification/blob/main/VGG_16_K_Fold_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFzHVCRHjWic",
        "outputId": "138f0144-d360-4e6e-f0a9-78c590f3d983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook uses 10 fold classification. The VGG-16/2 model has achieved an accuracy of 63%."
      ],
      "metadata": {
        "id": "bpy5iaTZR-50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def read_and_resize_image(image_path, target_size=(500, 500)):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image is successfully loaded\n",
        "    if image is None:\n",
        "        print(f\"Error loading image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Check if the image size is valid\n",
        "    if image.size == 0:\n",
        "        print(f\"Error resizing image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    image = cv2.resize(image, target_size)\n",
        "    return image\n",
        "\n",
        "# Function to create the mean aggregate image\n",
        "def create_mean_image(image_paths):\n",
        "    # Read and resize all images\n",
        "    images = [read_and_resize_image(image_path) for image_path in image_paths]\n",
        "\n",
        "    # Convert images to NumPy array\n",
        "    images_array = np.array(images)\n",
        "\n",
        "    # Calculate the mean along the first axis (axis=0) to get the aggregate image\n",
        "    mean_image = np.mean(images_array, axis=0).astype(np.uint8)\n",
        "    return mean_image\n",
        "\n",
        "# Function to save the GAF image\n",
        "def save_gaf_image(gaf_image, label, index):\n",
        "    folder_path = f\"/content/drive/MyDrive/New_Folder_TEST\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Save the image using OpenCV instead of Matplotlib\n",
        "    cv2.imwrite(f\"{folder_path}/{index}.png\", gaf_image)\n",
        "\n",
        "labels_df = pd.read_csv('/content/drive/MyDrive/y_test.csv', header=None)\n",
        "\n",
        "for index, row in labels_df.iterrows():\n",
        "        label = row[0]\n",
        "\n",
        "        image_paths = [os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_Y_GYRO', f\"Class{label}\", f\"{index+1}.png\"), os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_Y_ACC', f\"Class{label}\", f\"{index+1}.png\"), os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_Z_GYRO', f\"Class {label}\", f\"{index+1}.png\"), os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_Z_ACC', f\"Class {label}\", f\"{index+1}.png\"), os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_X_GYRO', f\"Class {label}\", f\"{index+1}.png\"), os.path.join('/content/drive/MyDrive/GAF_SUM_TEST_X_ACC', f\"Class {label}\", f\"{index+1}.png\")]\n",
        "\n",
        "        # Create the mean aggregate image\n",
        "        mean_image = create_mean_image(image_paths)\n",
        "\n",
        "        # Adjust pixel values for display\n",
        "        normalized_mean_image = cv2.normalize(mean_image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        # Save the mean image using the provided function\n",
        "        save_gaf_image(normalized_mean_image, label, index+1)"
      ],
      "metadata": {
        "id": "fBN9OMRy5bIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "\n",
        "# Load the labels without a header\n",
        "y = pd.read_csv('/content/drive/MyDrive/y_train.csv', header=None)\n",
        "\n",
        "# Convert labels to string\n",
        "y = y.astype(str)\n",
        "\n",
        "# Define the image size and batch size\n",
        "img_size = (128, 128)  # Adjust the size as per your requirement\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create DataFrame with image paths and labels\n",
        "df = pd.DataFrame({\n",
        "    'filename': ['/content/drive/MyDrive/New_Folder/{}.png'.format(i) for i in range(1, len(y) + 1)],\n",
        "    'label': y[0]\n",
        "})\n",
        "\n",
        "# Initialize StratifiedKFold with 10 folds\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(df['filename'], df['label'])):\n",
        "    print(f\"Training on fold {fold + 1}...\")\n",
        "\n",
        "    # Create train and validation data generators\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        df.iloc[train_index],\n",
        "        x_col='filename',\n",
        "        y_col='label',\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',  # Assuming it's a classification task\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_dataframe(\n",
        "        df.iloc[val_index],\n",
        "        x_col='filename',\n",
        "        y_col='label',\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "    )\n",
        "\n",
        "    # Build the CNN model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(len(y[0].unique()), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_acc = model.evaluate(val_generator)\n",
        "    print(f'Validation Accuracy on fold {fold + 1}: {val_acc}')\n",
        "\n",
        "    # Optionally, you can save the model for each fold\n",
        "    model.save(f'model_fold_{fold + 1}.h5')\n",
        "    val_loss, val_acc = model.evaluate(val_generator)\n",
        "    print(f'Validation Accuracy on fold {fold + 1}: {val_acc}')\n",
        "\n",
        "    # Store performance metrics\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "mean_val_accuracy = np.mean(val_accuracies)\n",
        "std_val_accuracy = np.std(val_accuracies)\n",
        "\n",
        "mean_val_loss = np.mean(val_losses)\n",
        "std_val_loss = np.std(val_losses)\n",
        "\n",
        "print(f'Mean Validation Accuracy: {mean_val_accuracy}, Std: {std_val_accuracy}')\n",
        "print(f'Mean Validation Loss: {mean_val_loss}, Std: {std_val_loss}')\n"
      ],
      "metadata": {
        "id": "l9ERA3p7Y8Bg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnZGGhMq+SHHFO7CHIKzsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}